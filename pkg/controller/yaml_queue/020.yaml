apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    data_size: "15"
    epoch: "129"
    model_name: llama3
  creationTimestamp: null
  name: "020"
spec:
  backoffLimit: 4
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - args:
        - --model-name
        - llama3
        - --model-path
        - /cyq/models/shenzhi-wang/Llama3.1-8B-Chinese-Chat
        command:
        - python
        - /cyq/test_demo/llm_cycle.py
        image: bencyq/llm_with_flask:202410242310
        name: llama3
        resources:
          limits:
            k8s.amazonaws.com/vgpu: "1"
        volumeMounts:
        - mountPath: /cyq
          name: cyq-volume
        - mountPath: /tmp/nvidia-mps
          name: nvidia-mps
      hostIPC: true
      nodeSelector:
        kubernetes.io/hostname: node16
      restartPolicy: Never
      volumes:
      - hostPath:
          path: /data/cyq
          type: Directory
        name: cyq-volume
      - hostPath:
          path: /tmp/nvidia-mps
        name: nvidia-mps
status: {}
