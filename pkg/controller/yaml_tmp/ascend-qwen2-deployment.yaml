apiVersion: apps/v1
kind: Deployment
metadata:
  name: ascend-qwen2-deployment
  labels:
    app: ascend-qwen2
spec:
  replicas: 1 
  selector:
    matchLabels:
      app: ascend-qwen2
  template:
    metadata:
      labels:
        app: ascend-qwen2
      annotations:
        hami.io/resource-pool: poc
    spec:
      containers:
        - name: ascend-qwen2
          image: dockerhub.kubekey.local/vast/xcore_llm:20241105
          #command: ["bash", "-c", "sleep 80000"]
          #command: ["bash", "-c", "chmod +x /workspace/SigInfer/start.sh && /workspace/SigInfer/start.sh"]
          command: ["bash", "-c", "source /root/.bashrc  && python3 /workspace/xcore-llm/src/api_server/api_server.py --model Ascend --tokenizer /workspace/Qwen2-7B-Instruct/ --tensor-parallel-size 1 --port 8000 --gpu-memory-utilization 0.7 --max-seq-len 2048 --max-num-seqs 2"]
          resources:
            limits:
              huawei.com/Ascend910B: 1
          volumeMounts:
            - name: model
              mountPath: /workspace/Qwen2-7B-Instruct
            - mountPath: /dev/shm
              name: dshm
          ports:
            - containerPort: 2050
            - containerPort: 8000
              hostPort: 8000
          securityContext:
            capabilities:
              add: ["ALL"]
      hostPID: true
      volumes:
        - name: model
          hostPath:
            path: /home/Qwen2-7B-Instruct
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 6Gi