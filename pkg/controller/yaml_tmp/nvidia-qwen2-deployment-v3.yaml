apiVersion: apps/v1
kind: Deployment
metadata:
  name: nvidia-qwen2-deployment
  labels:
    app: nvidia-qwen2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nvidia-qwen2
  template:
    metadata:
      labels:
        app: nvidia-qwen2
      annotations:
        hami.io/resource-pool: poc
        #nvidia.com/nouse-gpuuuid: "GPU-c2b9ceb0-5b8e-1d92-8497-1032a15da0f2"
        #nvidia.com/use-gputype: "NVIDIA-NVIDIA GeForce RTX 4090"
    spec:
      containers:
        - name: nvidia-qwen2
          image: dockerhub.kubekey.local/vast/nvidia_siginfer_qwen2:v1.0.1
          #command: ["bash", "-c", "sleep 80000"]
          command: ["bash", "-c", "chmod +x /workspace/SigInfer/start.sh && sed -i 's#Nvidia#Nvidia-27.154.1.18#g' /workspace/SigInfer/start.sh && source /root/.bashrc && sh /workspace/SigInfer/start.sh"]
          #command: ["bash", "-c", "source /root/.bashrc && python3 api_server/api_server.py --model Nvidia --tokenizer /home/weight/Qwen2-7B-Instruct/ --tensor-parallel-size 1 --port 8000 --max-seq-len 1024"]
          resources:
            limits:
              cpu: '8'
              memory: 16Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: model
              mountPath: /home/weight/Qwen2-7B-Instruct
            - mountPath: /dev/shm
              name: dshm
          ports:
            - containerPort: 2050
            - containerPort: 8000
              hostPort: 8000
          securityContext:
            capabilities:
              add: ["ALL"]
      hostPID: true
      volumes:
        - name: model
          hostPath:
            path: /root/vast/model/Qwen2-7B-Instruct
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 6Gi